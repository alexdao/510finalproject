\documentclass{article}
\setlength{\parskip}{\baselineskip}%

\title{CS510 Project Update: Detailed Diagnostic Distributed File System}
\date{\today}
\author{Alex Dao, Jiawei Zhang, and Danny Oh}

\usepackage{amsmath}
\usepackage[margin=1.5in]{geometry}
\usepackage{setspace}
\usepackage{hyperref}

\begin{document}
\maketitle

\section{Problem}
Recently, there has been a large shift towards storing large amounts of data in remote data centers, oftentimes distributed across many locations and hardware. Many current distributed file systems offer a layer of abstraction between a user and the distribution of the data, so that the user only experiences a file system like that on a Unix machine. However, in some cases, it would be useful for the user to actively see specifically which workers hold the data within a file system and how many concurrent users are requesting access to that file. 

Such a file system would provide important diagnostic information for job scheduling. If a user had a batch of jobs to be completed, they could be ordered in such a way that minimized the number of conflicts with concurrent users, based on the data provided by the DDDFS. Additionally, such data could be collected on the client-side for analysis on the usage the data within the file system, which could prove useful.

A sub-problem of distributed file systems are their performance, especially with regards to variable-size files. The Google File System\textsuperscript{[1]}, for example, is highly optimized for large files. Using such a system with small files is both space-inefficient and slow. 

\section{Goals}
\begin{enumerate}
\item Ability to query detailed read/write information over a variable time period (last n accesses) or a specific time period
\item Implement re-balancing algorithms on the master node to optimize for future reads or writes, based on the previous item. For example, if there happened to be numerous reads on Sundays at noon, then that data could be automatically replicated on more servers for better performance for concurrent accesses. This allows for increased performance on a smaller amount of hardware. 
\item Ability to query information about specific nodes that data is stored on, such as average file access latency, hardware type, location, etc. 
\item Implement basic functionality for this diagnostic layer on an existing data structure store (Redis)
\end{enumerate}

\section{Progress So Far}

\subsection{Infrastructure}

The master server that handles all requests was created using Spark, a lightweight Java web framework. Requests (read, write, or metadata modification) are sent through the HTTP protocol. HTTP was chosen over FTP due to simplicity in implementation as well as potential pipelinability. 

Just as in GFS, all metadata is stored in RAM and periodically flushed to disk. Redis (with Jedis Java driver) was used. Redis offers fast hashtable, list, and set storage, which are essential to our DDDFS implementation. 

Java was chosen over C and C++ for its ease of use. 

\subsection{Implementation}

To implement the master server, the following Redis maps were used:

\begin{enumerate}
\item read: LRU list of the filenames of the last 1000 read requests
\item write: LRU list of the filenames of the last 1000 write requests
\item originalSlave: hashtable mapping from path to slave server written to upon file creation
\item slaves: hashtable mapping from path to slave servers that contain copies of the file, including the original slave
\item slaveUses: LRU list of the last 1000 slaves accesses for any reason
\end{enumerate}

\subsubsection{Writing}

Writing is the most simple operation. If the file being written does not exist, the least popular existing slave is chosen and the file is written to the slave. 

Whether the file previously exists or not, the write is pushed onto the 'write' Redis list. 

If the file previously existed, the file is disseminated to all slaves that also own the file. 

\subsubsection{Reading}

For every read request, the read is pushed onto the 'read' Redis list. 

There are two possible ways to determine the slave from which to read from:

\begin{enumerate}
\item Choose a random slave from the 'slaves' hashtable for the file
\item Choose the least popular slave contained within the 'slaves' hashtable for the file
\end{enumerate}

The first method allows for faster slave-selection but can result in bottlenecking a potentially busy slave. The second method allows for best balancing of slave usage but it can potentially be resource-intensive to find the least popular slave. 

\subsubsection{Modification}

The current implementation does not support file metadata modification such as changing a file's location. These features' implementations are orthogonal to DDDFS's desired features and are not a priority. 

\subsubsection{Balancing}

The plan is to only perform balancing for read requests. The read LRU list is used to generate a map from file to count (in the last 1000 requests). The current implementation attempts to maintain 1 slave for every 100 reads (arbitrarily set) + 1. For example, for 150 reads in the last 1000 requests will result in desiring 1+1=2 slaves.  

If the desired number of slaves is greater than the current number of slaves, slaves are added from the pool of least popular slaves (in terms of total requests). 

If the desired number of slaves is less than the current number of slaves, (desired - current) slaves are arbitrarily removed for the file. 

Either way, the changes then propagate to the added/removed slaves. 

\subsubsection{Consistency}

As our implementation relies on Redis's locks and doesn't utilize the order in which requests are applied, the entire system's consistency is eventual. 


\section{Related Work}
\subsection{HDFS}
The Hadoop Distributed File System (HDFS) is a distributed, fault-tolerant file system designed to run on low-cost commodity hardware, which is accessed via command line inputs. This file system is built around a write-once-read-many model, similar to the applications that our project is optimizing for \cite{HDFS}.

HDFS is comprised of interconnected clusters of nodes. Each cluster contains a single \emph{NameNode} that manages file system namespace and regulates client access to files, similar to many other file systems following a master-slave approach \cite{HDFS}. To simplify things, our research project will focus on a distributed file system where all I/O must go through a single master. 

Data nodes in HDFS support rebalancing if the free space on a data node falls too low or if a sudden increase in demand for a given file occurs. However, no functionality exists for pre-emptive rebalancing based on past history of reads. In addition, since most data nodes within a cluster typically reside in the same location, there is no consideration given towards balancing to optimize for variances in performance for different data centers, such as network latency, hardware performance, etc. Our project focuses on the analysis and load balancing which account for these factors. 

\newpage
\begin{thebibliography}{10}

\bibitem{HDFS} Jeffrey J. Hanson
\newblock An introduction to the Hadoop Distributed File System
\newblock \textit{developerWorks}
\newblock IBM. 1 February 2011. 

\end{thebibliography}

\end{document}