\documentclass{article}
\setlength{\parskip}{\baselineskip}%

\title{CS510 Project Update: Detailed Diagnostic Distributed File System}
\date{\today}
\author{Alex Dao, Jiawei Zhang, and Danny Oh}

\usepackage{amsmath}
\usepackage[margin=1.5in]{geometry}
\usepackage{setspace}
\usepackage{hyperref}

\begin{document}
\maketitle

\section{Problem}
Recently, there has been a large shift towards storing large amounts of data in remote data centers, oftentimes distributed across many locations and hardware. Many current distributed file systems offer a layer of abstraction between a user and the distribution of the data, so that the user only experiences a file system like that on a Unix machine. However, in some cases, it would be useful for the user to actively see specifically which workers hold the data within a file system and how many concurrent users are requesting access to that file. 

Such a file system would provide important diagnostic information for job scheduling. If a user had a batch of jobs to be completed, they could be ordered in such a way that minimized the number of conflicts with concurrent users, based on the data provided by the DDDFS. Additionally, such data could be collected on the client-side for analysis on the usage the data within the file system, which could prove useful.

A sub-problem of distributed file systems are their performance, especially with regards to variable-size files. The Google File System\textsuperscript{[1]}, for example, is highly optimized for large files. Using such a system with small files is both space-inefficient and slow. 

\section{Goals}
\begin{enumerate}
\item Ability to query detailed read/write information over a variable time period or a specific time period
\item Implement re-balancing algorithms on the master node to optimize for future reads or writes, based on the previous item. For example, if there happened to be numerous reads on Sundays at noon, then that data could be automatically replicated on more servers for better performance for concurrent accesses. This allows for increased performance on a smaller amount of hardware. 
\item Ability to query information about specific nodes that data is stored on, such as average file access latency, hardware type, location, etc. 
\item Implement basic functionality for this diagnostic layer on an existing data structure store (Redis)
\end{enumerate}

\section{Progress So Far}


\section{Related Work}
\subsection{HDFS}
The Hadoop Distributed File System (HDFS) is a distributed, fault-tolerant file system designed to run on low-cost commodity hardware, which is accessed via command line inputs. This file system is built around a write-once-read-many model, similar to the applications that our project is optimizing for \cite{HDFS}.

HDFS is comprised of interconnected clusters of nodes. Each cluster contains a single \emph{NameNode} that manages file system namespace and regulates client access to files, similar to many other file systems following a master-slave approach \cite{HDFS}. To simplify things, our research project will focus on a distributed file system where all I/O must go through a single master. 

Data nodes in HDFS support rebalancing if the free space on a data node falls too low or if a sudden increase in demand for a given file occurs. However, no functionality exists for pre-emptive rebalancing based on past history of reads. In addition, since most data nodes within a cluster typically reside in the same location, there is no consideration given towards balancing to optimize for variances in performance for different data centers, such as network latency, hardware performance, etc. Our project focuses on the analysis and load balancing which account for these factors. 

\newpage
\begin{thebibliography}{10}

\bibitem{HDFS} Jeffrey J. Hanson
\newblock An introduction to the Hadoop Distributed File System
\newblock \textit{developerWorks}
\newblock IBM. 1 February 2011. 

\end{thebibliography}

\end{document}